{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "from datasets import load_dataset\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3), (20, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:20]\")\n",
    "val = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:5]\")\n",
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # normalize characters\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation, non-ascii characters, and other patterns\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    text = re.sub(\"(\\\\t)\", \" \", str(text)).lower()\n",
    "    text = re.sub(\"(\\\\r)\", \" \", str(text)).lower()\n",
    "    text = re.sub(\"(\\\\n)\", \" \", str(text)).lower()\n",
    "    text = re.sub(\"(\\.\\s+)\", \" \", str(text)).lower()\n",
    "    text = re.sub(\"(\\-\\s+)\", \" \", str(text)).lower()\n",
    "    text = re.sub(\"(\\:\\s+)\", \" \", str(text)).lower()\n",
    "\n",
    "    # remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_articles = []\n",
    "for sent in train['article']:\n",
    "    s = clean(sent)\n",
    "    train_clean_articles.append(s)\n",
    "\n",
    "train_clean_summaries = []\n",
    "for sent in train['highlights']:\n",
    "    s = clean(sent)\n",
    "    train_clean_summaries.append(s)\n",
    "\n",
    "\n",
    "\n",
    "val_clean_articles = []\n",
    "for sent in val['article']:\n",
    "    s = clean(sent)\n",
    "    val_clean_articles.append(s)\n",
    "\n",
    "val_clean_summaries = []\n",
    "for sent in val['highlights']:\n",
    "    s = clean(sent)\n",
    "    val_clean_summaries.append(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.DataFrame()\n",
    "train_clean['train_cleaned_article'] = pd.Series(train_clean_articles)\n",
    "train_clean['train_cleaned_summary'] = pd.Series(train_clean_summaries)\n",
    "\n",
    "val_clean = pd.DataFrame()\n",
    "val_clean['val_cleaned_article'] = pd.Series(val_clean_articles)\n",
    "val_clean['val_cleaned_summary'] = pd.Series(val_clean_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Check how much % of text have 0-1000 words\n",
    "cnt = 0\n",
    "for i in train_clean['train_cleaned_article']:\n",
    "    if len(i.split()) <= 1000:\n",
    "        cnt = cnt + 1\n",
    "print(cnt / len(train_clean['train_cleaned_article']))\n",
    "\n",
    "# Check how much % of summaries have 0-75 words\n",
    "cnt = 0\n",
    "for i in train_clean['train_cleaned_summary']:\n",
    "    if len(i.split()) <= 75:\n",
    "        cnt = cnt + 1\n",
    "print(cnt / len(train_clean['train_cleaned_summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>london england reuters harry potter star danie...</td>\n",
       "      <td>harry potter star daniel radcliffe gets 20m fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>editors note behind scenes series cnn correspo...</td>\n",
       "      <td>mentally ill inmates miami housed forgotten fl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  london england reuters harry potter star danie...   \n",
       "1  editors note behind scenes series cnn correspo...   \n",
       "\n",
       "                                             summary  \n",
       "0  harry potter star daniel radcliffe gets 20m fo...  \n",
       "1  mentally ill inmates miami housed forgotten fl...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_article_len = 1000\n",
    "max_summary_len = 75\n",
    "\n",
    "train_cleaned_article = np.array(train_clean['train_cleaned_article'])\n",
    "train_cleaned_summary= np.array(train_clean['train_cleaned_summary'])\n",
    "\n",
    "short_article = []\n",
    "short_summary = []\n",
    "\n",
    "for i in range(len(train_cleaned_article)):\n",
    "    if len(train_cleaned_summary[i].split()) <= max_summary_len and len(train_cleaned_article[i].split()) <= max_article_len:\n",
    "        short_article.append(train_cleaned_article[i])\n",
    "        short_summary.append(train_cleaned_summary[i])\n",
    "        \n",
    "post_train_clean = pd.DataFrame({'article': short_article,'summary': short_summary})\n",
    "\n",
    "post_train_clean.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>london england reuters harry potter star danie...</td>\n",
       "      <td>senstart harry potter star daniel radcliffe ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>editors note behind scenes series cnn correspo...</td>\n",
       "      <td>senstart mentally ill inmates miami housed for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  london england reuters harry potter star danie...   \n",
       "1  editors note behind scenes series cnn correspo...   \n",
       "\n",
       "                                             summary  \n",
       "0  senstart harry potter star daniel radcliffe ge...  \n",
       "1  senstart mentally ill inmates miami housed for...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add start and end tokens\n",
    "\n",
    "post_train_clean['summary'] = post_train_clean['summary'].apply(lambda x: 'senstart ' + x \\\n",
    "        + ' senend')\n",
    "\n",
    "post_train_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnnshare gift multiplied may sound like esoter...</td>\n",
       "      <td>zully broussard decided give kidney stranger n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnnon 6th april 1996 san jose clash dc united ...</td>\n",
       "      <td>20th mls season begins weekend league changed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  cnnshare gift multiplied may sound like esoter...   \n",
       "1  cnnon 6th april 1996 san jose clash dc united ...   \n",
       "\n",
       "                                             summary  \n",
       "0  zully broussard decided give kidney stranger n...  \n",
       "1  20th mls season begins weekend league changed ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_cleaned_article = np.array(val_clean['val_cleaned_article'])\n",
    "val_cleaned_summary= np.array(val_clean['val_cleaned_summary'])\n",
    "\n",
    "short_article = []\n",
    "short_summary = []\n",
    "\n",
    "for i in range(len(val_cleaned_article)):\n",
    "    if len(val_cleaned_summary[i].split()) <= max_summary_len and len(val_cleaned_article[i].split()) <= max_article_len:\n",
    "        short_article.append(val_cleaned_article[i])\n",
    "        short_summary.append(val_cleaned_summary[i])\n",
    "        \n",
    "post_val_clean = pd.DataFrame({'article': short_article,'summary': short_summary})\n",
    "\n",
    "post_val_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnnshare gift multiplied may sound like esoter...</td>\n",
       "      <td>senstart zully broussard decided give kidney s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnnon 6th april 1996 san jose clash dc united ...</td>\n",
       "      <td>senstart 20th mls season begins weekend league...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  cnnshare gift multiplied may sound like esoter...   \n",
       "1  cnnon 6th april 1996 san jose clash dc united ...   \n",
       "\n",
       "                                             summary  \n",
       "0  senstart zully broussard decided give kidney s...  \n",
       "1  senstart 20th mls season begins weekend league...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add start and end tokens\n",
    "\n",
    "post_val_clean['summary'] = post_val_clean['summary'].apply(lambda x: 'senstart ' + x \\\n",
    "        + ' senend')\n",
    "\n",
    "post_val_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(post_train_clean['article'])\n",
    "x_val = list(post_val_clean['article'])\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  77.8688524590164\n"
     ]
    }
   ],
   "source": [
    "thresh = 3\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 595\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "x_tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "x_train_seq = x_tokenizer.texts_to_sequences(x_train) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# Pad zero upto maximum length\n",
    "x_train = pad_sequences(x_train_seq,  maxlen=max_article_len, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=max_article_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(post_train_clean['summary'])\n",
    "y_val = list(post_val_clean['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 95.69377990430623\n",
      "Size of vocabulary in Y = 19\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer on testing data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "\n",
    "thresh = 3\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
    "\n",
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "y_train_seq = y_tokenizer.texts_to_sequences(y_train) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# Pad zero upto maximum length\n",
    "y_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 200\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_article_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb = Embedding(x_voc, embedding_dim,\n",
    "                    trainable=True)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder LSTM 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
    "                     return_sequences=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using encoder_states as the initial state\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
    "                    return_state=True, dropout=0.4,\n",
    "                    recurrent_dropout=0.2)\n",
    "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
    "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1000)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1000, 100)            59500     ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 1000, 200),          240800    ['embedding[0][0]']           \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 1000, 200),          320800    ['lstm[0][0]']                \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 100)            1900      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 1000, 200),          320800    ['lstm_1[0][0]']              \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, None, 200),          240800    ['embedding_1[0][0]',         \n",
      "                              (None, 200),                           'lstm_2[0][1]',              \n",
      "                              (None, 200)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, None, 19)             3819      ['lstm_3[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1188419 (4.53 MB)\n",
      "Trainable params: 1188419 (4.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 44s 44s/step - loss: 2.9451 - val_loss: 2.6775\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 105s 105s/step - loss: 2.6900 - val_loss: 2.0616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b5f47fcc90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [x_train, y_train[:, :-1]],\n",
    "    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n",
    "    epochs=7,\n",
    "    callbacks=[es],\n",
    "    batch_size=128,\n",
    "    validation_data=([x_val, y_val[:, :-1]],\n",
    "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
    "                     , 1:]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'senstart': 1,\n",
       " 'senend': 2,\n",
       " 'says': 3,\n",
       " 'president': 4,\n",
       " 'bush': 5,\n",
       " 'new': 6,\n",
       " 'first': 7,\n",
       " 'found': 8,\n",
       " 'us': 9,\n",
       " 'cnn': 10,\n",
       " 'colonoscopy': 11,\n",
       " 'nfl': 12,\n",
       " 'two': 13,\n",
       " 'killed': 14,\n",
       " 'snow': 15,\n",
       " 'say': 16,\n",
       " 'july': 17,\n",
       " 'bees': 18,\n",
       " 'potter': 19,\n",
       " 'turns': 20,\n",
       " 'monday': 21,\n",
       " 'five': 22,\n",
       " 'hes': 23,\n",
       " 'driver': 24,\n",
       " 'wednesday': 25,\n",
       " 'powers': 26,\n",
       " 'transferred': 27,\n",
       " 'vice': 28,\n",
       " 'routine': 29,\n",
       " 'chief': 30,\n",
       " 'falcons': 31,\n",
       " 'without': 32,\n",
       " 'pay': 33,\n",
       " 'vick': 34,\n",
       " 'violence': 35,\n",
       " 'prostitution': 36,\n",
       " 'group': 37,\n",
       " 'children': 38,\n",
       " '2002': 39,\n",
       " 'press': 40,\n",
       " 'empty': 41,\n",
       " 'weapon': 42,\n",
       " 'home': 43,\n",
       " 'london': 44,\n",
       " 'friday': 45,\n",
       " 'club': 46,\n",
       " '2004': 47,\n",
       " 'since': 48,\n",
       " 'oakland': 49,\n",
       " 'police': 50,\n",
       " 'miles': 51,\n",
       " '15': 52,\n",
       " 'beckham': 53,\n",
       " 'contract': 54,\n",
       " 'collapse': 55,\n",
       " 'disorder': 56,\n",
       " 'bank': 57,\n",
       " 'harry': 58,\n",
       " 'star': 59,\n",
       " 'daniel': 60,\n",
       " 'radcliffe': 61,\n",
       " 'gets': 62,\n",
       " '20m': 63,\n",
       " 'fortune': 64,\n",
       " '18': 65,\n",
       " 'young': 66,\n",
       " 'actor': 67,\n",
       " 'plans': 68,\n",
       " 'fritter': 69,\n",
       " 'cash': 70,\n",
       " 'away': 71,\n",
       " 'radcliffes': 72,\n",
       " 'earnings': 73,\n",
       " 'films': 74,\n",
       " 'held': 75,\n",
       " 'trust': 76,\n",
       " 'fund': 77,\n",
       " 'mentally': 78,\n",
       " 'ill': 79,\n",
       " 'inmates': 80,\n",
       " 'miami': 81,\n",
       " 'housed': 82,\n",
       " 'forgotten': 83,\n",
       " 'floorjudge': 84,\n",
       " 'steven': 85,\n",
       " 'leifman': 86,\n",
       " 'result': 87,\n",
       " 'avoidable': 88,\n",
       " 'felonieswhile': 89,\n",
       " 'tours': 90,\n",
       " 'facility': 91,\n",
       " 'patient': 92,\n",
       " 'shouts': 93,\n",
       " 'son': 94,\n",
       " 'presidentleifman': 95,\n",
       " 'system': 96,\n",
       " 'unjust': 97,\n",
       " 'fighting': 98,\n",
       " 'change': 99,\n",
       " 'thought': 100,\n",
       " 'going': 101,\n",
       " 'die': 102,\n",
       " 'man': 103,\n",
       " 'pickup': 104,\n",
       " 'truck': 105,\n",
       " 'folded': 106,\n",
       " 'half': 107,\n",
       " 'cut': 108,\n",
       " 'face': 109,\n",
       " 'probably': 110,\n",
       " '30': 111,\n",
       " '35foot': 112,\n",
       " 'free': 113,\n",
       " 'fallminnesota': 114,\n",
       " 'bridge': 115,\n",
       " 'collapsed': 116,\n",
       " 'rush': 117,\n",
       " 'hour': 118,\n",
       " 'small': 119,\n",
       " 'polyps': 120,\n",
       " 'procedure': 121,\n",
       " 'none': 122,\n",
       " 'worrisome': 123,\n",
       " 'spokesman': 124,\n",
       " 'reclaims': 125,\n",
       " 'undergoes': 126,\n",
       " 'camp': 127,\n",
       " 'david': 128,\n",
       " 'atlanta': 129,\n",
       " 'owner': 130,\n",
       " 'critical': 131,\n",
       " 'michael': 132,\n",
       " 'vicks': 133,\n",
       " 'conduct': 134,\n",
       " 'suspends': 135,\n",
       " 'quarterback': 136,\n",
       " 'indefinitely': 137,\n",
       " 'admits': 138,\n",
       " 'funding': 139,\n",
       " 'dogfighting': 140,\n",
       " 'operation': 141,\n",
       " 'gamble': 142,\n",
       " 'due': 143,\n",
       " 'federal': 144,\n",
       " 'court': 145,\n",
       " 'future': 146,\n",
       " 'remains': 147,\n",
       " 'uncertain': 148,\n",
       " 'parents': 149,\n",
       " 'beam': 150,\n",
       " 'pride': 151,\n",
       " 'cant': 152,\n",
       " 'stop': 153,\n",
       " 'smiling': 154,\n",
       " 'outpouring': 155,\n",
       " 'support': 156,\n",
       " 'mom': 157,\n",
       " 'happy': 158,\n",
       " 'didnt': 159,\n",
       " 'know': 160,\n",
       " 'doburn': 161,\n",
       " 'center': 162,\n",
       " 'offered': 163,\n",
       " 'provide': 164,\n",
       " 'treatment': 165,\n",
       " 'reconstructive': 166,\n",
       " 'surgeries': 167,\n",
       " 'dad': 168,\n",
       " 'anything': 169,\n",
       " 'youssif': 170,\n",
       " 'aid': 171,\n",
       " 'workers': 172,\n",
       " 'increased': 173,\n",
       " 'cost': 174,\n",
       " 'living': 175,\n",
       " 'drive': 176,\n",
       " 'women': 177,\n",
       " 'working': 178,\n",
       " 'raise': 179,\n",
       " 'awareness': 180,\n",
       " 'problem': 181,\n",
       " 'iraqs': 182,\n",
       " 'political': 183,\n",
       " 'leaders': 184,\n",
       " 'iraqi': 185,\n",
       " 'mothers': 186,\n",
       " 'tell': 187,\n",
       " 'turned': 188,\n",
       " 'help': 189,\n",
       " 'feed': 190,\n",
       " 'everything': 191,\n",
       " 'one': 192,\n",
       " 'woman': 193,\n",
       " 'tomas': 194,\n",
       " 'medina': 195,\n",
       " 'caracas': 196,\n",
       " 'fugitive': 197,\n",
       " 'drug': 198,\n",
       " 'trafficking': 199,\n",
       " 'indictment': 200,\n",
       " 'el': 201,\n",
       " 'negro': 202,\n",
       " 'acacio': 203,\n",
       " 'allegedly': 204,\n",
       " 'helped': 205,\n",
       " 'manage': 206,\n",
       " 'extensive': 207,\n",
       " 'cocaine': 208,\n",
       " 'network': 209,\n",
       " 'justice': 210,\n",
       " 'department': 211,\n",
       " 'indicted': 212,\n",
       " 'colombian': 213,\n",
       " 'military': 214,\n",
       " 'attack': 215,\n",
       " 'guerrilla': 216,\n",
       " 'encampment': 217,\n",
       " 'tony': 218,\n",
       " 'battle': 219,\n",
       " 'cancer': 220,\n",
       " 'win': 221,\n",
       " 'job': 222,\n",
       " 'secretary': 223,\n",
       " 'dream': 224,\n",
       " 'leaving': 225,\n",
       " 'september': 226,\n",
       " '14': 227,\n",
       " 'succeeded': 228,\n",
       " 'dana': 229,\n",
       " 'perino': 230,\n",
       " 'antitank': 231,\n",
       " 'front': 232,\n",
       " 'jersey': 233,\n",
       " 'device': 234,\n",
       " 'handed': 235,\n",
       " 'army': 236,\n",
       " 'ordnance': 237,\n",
       " 'disposal': 238,\n",
       " 'unit': 239,\n",
       " 'capable': 240,\n",
       " 'reloaded': 241,\n",
       " 'experts': 242,\n",
       " 'address': 243,\n",
       " 'veterans': 244,\n",
       " 'foreign': 245,\n",
       " 'wars': 246,\n",
       " 'withdrawing': 247,\n",
       " 'vietnam': 248,\n",
       " 'emboldened': 249,\n",
       " 'todays': 250,\n",
       " 'terrorists': 251,\n",
       " 'speech': 252,\n",
       " 'latest': 253,\n",
       " 'white': 254,\n",
       " 'house': 255,\n",
       " 'attempt': 256,\n",
       " 'try': 257,\n",
       " 'reframe': 258,\n",
       " 'debate': 259,\n",
       " 'iraq': 260,\n",
       " 'cars': 261,\n",
       " 'loaded': 262,\n",
       " 'gasoline': 263,\n",
       " 'nails': 264,\n",
       " 'abandoned': 265,\n",
       " '52': 266,\n",
       " 'people': 267,\n",
       " '7': 268,\n",
       " '2005': 269,\n",
       " 'bombs': 270,\n",
       " 'exploded': 271,\n",
       " 'bus': 272,\n",
       " 'trains': 273,\n",
       " 'british': 274,\n",
       " 'capital': 275,\n",
       " 'wracked': 276,\n",
       " 'ira': 277,\n",
       " 'years': 278,\n",
       " 'werder': 279,\n",
       " 'bremen': 280,\n",
       " 'record': 281,\n",
       " '107': 282,\n",
       " 'million': 283,\n",
       " 'carlos': 284,\n",
       " 'alberto': 285,\n",
       " 'brazilian': 286,\n",
       " 'midfielder': 287,\n",
       " 'champions': 288,\n",
       " 'league': 289,\n",
       " 'fc': 290,\n",
       " 'porto': 291,\n",
       " 'january': 292,\n",
       " 'loan': 293,\n",
       " 'fluminense': 294,\n",
       " 'saturday': 295,\n",
       " 'anesthetized': 296,\n",
       " 'last': 297,\n",
       " 'problems': 298,\n",
       " '2000': 299,\n",
       " 'customers': 300,\n",
       " 'electricity': 301,\n",
       " 'power': 302,\n",
       " 'company': 303,\n",
       " 'magnitude': 304,\n",
       " '42': 305,\n",
       " 'quake': 306,\n",
       " 'set': 307,\n",
       " 'alarms': 308,\n",
       " 'dispatcher': 309,\n",
       " 'fairly': 310,\n",
       " 'mild': 311,\n",
       " 'immediate': 312,\n",
       " 'reports': 313,\n",
       " 'injuries': 314,\n",
       " 'damage': 315,\n",
       " 'centered': 316,\n",
       " 'eastnortheast': 317,\n",
       " '36': 318,\n",
       " 'deep': 319,\n",
       " 'lady': 320,\n",
       " 'deeply': 321,\n",
       " 'saddened': 322,\n",
       " 'tragedy': 323,\n",
       " 'mine': 324,\n",
       " 'safety': 325,\n",
       " 'health': 326,\n",
       " 'administration': 327,\n",
       " 'weve': 328,\n",
       " 'run': 329,\n",
       " 'optionsthe': 330,\n",
       " 'six': 331,\n",
       " 'men': 332,\n",
       " 'trapped': 333,\n",
       " 'underground': 334,\n",
       " 'august': 335,\n",
       " '6': 336,\n",
       " 'seven': 337,\n",
       " 'bore': 338,\n",
       " 'holes': 339,\n",
       " 'drilled': 340,\n",
       " 'mountain': 341,\n",
       " 'signs': 342,\n",
       " 'life': 343,\n",
       " 'bomb': 344,\n",
       " 'victims': 345,\n",
       " 'waiting': 346,\n",
       " 'presidential': 347,\n",
       " 'visit': 348,\n",
       " 'blast': 349,\n",
       " 'went': 350,\n",
       " 'minutes': 351,\n",
       " 'presidents': 352,\n",
       " 'arrival': 353,\n",
       " 'algeria': 354,\n",
       " 'faces': 355,\n",
       " 'islamic': 356,\n",
       " 'insurgency': 357,\n",
       " 'al': 358,\n",
       " 'qaedaaffiliated': 359,\n",
       " 'claimed': 360,\n",
       " 'attacks': 361,\n",
       " 'agreed': 362,\n",
       " 'fiveyear': 363,\n",
       " 'los': 364,\n",
       " 'angeles': 365,\n",
       " 'galaxy': 366,\n",
       " 'took': 367,\n",
       " 'effect': 368,\n",
       " '1': 369,\n",
       " '2007': 370,\n",
       " 'former': 371,\n",
       " 'english': 372,\n",
       " 'captain': 373,\n",
       " 'meet': 374,\n",
       " 'unveil': 375,\n",
       " 'shirt': 376,\n",
       " 'number': 377,\n",
       " 'look': 378,\n",
       " 'footballer': 379,\n",
       " 'fashion': 380,\n",
       " 'icon': 381,\n",
       " 'global': 382,\n",
       " 'phenomenon': 383,\n",
       " 'colony': 384,\n",
       " 'millions': 385,\n",
       " 'scientists': 386,\n",
       " 'suspect': 387,\n",
       " 'virus': 388,\n",
       " 'may': 389,\n",
       " 'combine': 390,\n",
       " 'factors': 391,\n",
       " 'colonies': 392,\n",
       " 'cropped': 393,\n",
       " 'imported': 394,\n",
       " 'australia': 395,\n",
       " 'billion': 396,\n",
       " 'crops': 397,\n",
       " 'year': 398,\n",
       " 'dependent': 399,\n",
       " 'pollination': 400,\n",
       " 'savers': 401,\n",
       " 'leading': 402,\n",
       " 'uk': 403,\n",
       " 'mortgage': 404,\n",
       " 'lined': 405,\n",
       " 'accounts': 406,\n",
       " 'northern': 407,\n",
       " 'rock': 408,\n",
       " 'bailed': 409,\n",
       " 'england': 410,\n",
       " 'day': 411,\n",
       " 'earlier': 412,\n",
       " 'reassurances': 413,\n",
       " 'banks': 414,\n",
       " 'safe': 415,\n",
       " 'gone': 416,\n",
       " 'unheeded': 417,\n",
       " 'many': 418}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Models\n",
    "\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
    "                      state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "decoder_hidden_state_input = Input(shape=(max_article_len, latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
    "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
    "                      decoder_state_input_h, decoder_state_input_c],\n",
    "                      [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['senstart']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
    "                + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'senend':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'senend' or len(decoded_sentence.split()) \\\n",
    "            >= max_summary_len - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert sequence to summary\n",
    "def seq2summary(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0 and i != target_word_index['senstart'] and i \\\n",
    "            != target_word_index['senend']:\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "\n",
    "    return newString\n",
    "\n",
    "\n",
    "# To convert sequence to text\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: london england reuters harry potter star radcliffe reported million million 18 monday money wont radcliffe harry potter harry potter order around world young says cash away cars celebrity dont one people turn 18 car told australian earlier month dont think ill things like things pounds 18 radcliffe able see part six number one movie uk hell birthday ill sort said none first five potter held fund able says feet ground people say star goes told reporters last month try hard go way would latest boy harry potter order last two watch give latest life potter movie called boy son later year also appear december boys australian four boys earlier year made playing meanwhile even closer hes think im going sort game told reuters email friend 2007 reuters may \n",
      "Original summary: says first \n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted summary:  senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart\n",
      "\n",
      "\n",
      "Review: behind series cnn news behind inside jail many inmates mentally ill forgotten floor many mentally ill inmates miami miami cnn ninth floor forgotten floor inmates mental theyre appear court often face drug charges charges charges judge leifman says says often result police mentally ill people often wont theyre told police scene become less likely follow according leifman end ninth floor mentally getting real help theyre jail jail leifman well known miami justice mentally ill even exactly floor go inside forgotten floor first hard people feet thats look like theyre mentally ill patients injuring thats also leifman says people jails mentally ill says system result see ninth floor jail cells see two sometimes three men sometimes sometimes lying cells son president need get one man help way could reach white house leifman often system mental jail face charges become things miami later talk things got way mental patients leifman says 200 years ago people considered jails even charges considered society years says public mentally ill moved jails hospitals leifman says many mental hospitals patients go streets many cases says never got treatment leifman says half million people state mental hospitals number 90 percent people mental hospitals judge says hes working change many inmates would forgotten floor sent new mental health first step toward treatment leifman says start leifman says best part patients win families state money simply leifman justice email friend \n",
      "Original summary: says cnn says \n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted summary:  senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart senstart\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 2):\n",
    "    print('Review:', seq2text(x_train[i]))\n",
    "    print('Original summary:', seq2summary(y_train[i]))\n",
    "    print('Predicted summary:', decode_sequence(x_train[i].reshape(1,\n",
    "           max_article_len)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'senstart',\n",
       " 2: 'senend',\n",
       " 3: 'says',\n",
       " 4: 'president',\n",
       " 5: 'bush',\n",
       " 6: 'new',\n",
       " 7: 'first',\n",
       " 8: 'found',\n",
       " 9: 'us',\n",
       " 10: 'cnn',\n",
       " 11: 'colonoscopy',\n",
       " 12: 'nfl',\n",
       " 13: 'two',\n",
       " 14: 'killed',\n",
       " 15: 'snow',\n",
       " 16: 'say',\n",
       " 17: 'july',\n",
       " 18: 'bees',\n",
       " 19: 'potter',\n",
       " 20: 'turns',\n",
       " 21: 'monday',\n",
       " 22: 'five',\n",
       " 23: 'hes',\n",
       " 24: 'driver',\n",
       " 25: 'wednesday',\n",
       " 26: 'powers',\n",
       " 27: 'transferred',\n",
       " 28: 'vice',\n",
       " 29: 'routine',\n",
       " 30: 'chief',\n",
       " 31: 'falcons',\n",
       " 32: 'without',\n",
       " 33: 'pay',\n",
       " 34: 'vick',\n",
       " 35: 'violence',\n",
       " 36: 'prostitution',\n",
       " 37: 'group',\n",
       " 38: 'children',\n",
       " 39: '2002',\n",
       " 40: 'press',\n",
       " 41: 'empty',\n",
       " 42: 'weapon',\n",
       " 43: 'home',\n",
       " 44: 'london',\n",
       " 45: 'friday',\n",
       " 46: 'club',\n",
       " 47: '2004',\n",
       " 48: 'since',\n",
       " 49: 'oakland',\n",
       " 50: 'police',\n",
       " 51: 'miles',\n",
       " 52: '15',\n",
       " 53: 'beckham',\n",
       " 54: 'contract',\n",
       " 55: 'collapse',\n",
       " 56: 'disorder',\n",
       " 57: 'bank',\n",
       " 58: 'harry',\n",
       " 59: 'star',\n",
       " 60: 'daniel',\n",
       " 61: 'radcliffe',\n",
       " 62: 'gets',\n",
       " 63: '20m',\n",
       " 64: 'fortune',\n",
       " 65: '18',\n",
       " 66: 'young',\n",
       " 67: 'actor',\n",
       " 68: 'plans',\n",
       " 69: 'fritter',\n",
       " 70: 'cash',\n",
       " 71: 'away',\n",
       " 72: 'radcliffes',\n",
       " 73: 'earnings',\n",
       " 74: 'films',\n",
       " 75: 'held',\n",
       " 76: 'trust',\n",
       " 77: 'fund',\n",
       " 78: 'mentally',\n",
       " 79: 'ill',\n",
       " 80: 'inmates',\n",
       " 81: 'miami',\n",
       " 82: 'housed',\n",
       " 83: 'forgotten',\n",
       " 84: 'floorjudge',\n",
       " 85: 'steven',\n",
       " 86: 'leifman',\n",
       " 87: 'result',\n",
       " 88: 'avoidable',\n",
       " 89: 'felonieswhile',\n",
       " 90: 'tours',\n",
       " 91: 'facility',\n",
       " 92: 'patient',\n",
       " 93: 'shouts',\n",
       " 94: 'son',\n",
       " 95: 'presidentleifman',\n",
       " 96: 'system',\n",
       " 97: 'unjust',\n",
       " 98: 'fighting',\n",
       " 99: 'change',\n",
       " 100: 'thought',\n",
       " 101: 'going',\n",
       " 102: 'die',\n",
       " 103: 'man',\n",
       " 104: 'pickup',\n",
       " 105: 'truck',\n",
       " 106: 'folded',\n",
       " 107: 'half',\n",
       " 108: 'cut',\n",
       " 109: 'face',\n",
       " 110: 'probably',\n",
       " 111: '30',\n",
       " 112: '35foot',\n",
       " 113: 'free',\n",
       " 114: 'fallminnesota',\n",
       " 115: 'bridge',\n",
       " 116: 'collapsed',\n",
       " 117: 'rush',\n",
       " 118: 'hour',\n",
       " 119: 'small',\n",
       " 120: 'polyps',\n",
       " 121: 'procedure',\n",
       " 122: 'none',\n",
       " 123: 'worrisome',\n",
       " 124: 'spokesman',\n",
       " 125: 'reclaims',\n",
       " 126: 'undergoes',\n",
       " 127: 'camp',\n",
       " 128: 'david',\n",
       " 129: 'atlanta',\n",
       " 130: 'owner',\n",
       " 131: 'critical',\n",
       " 132: 'michael',\n",
       " 133: 'vicks',\n",
       " 134: 'conduct',\n",
       " 135: 'suspends',\n",
       " 136: 'quarterback',\n",
       " 137: 'indefinitely',\n",
       " 138: 'admits',\n",
       " 139: 'funding',\n",
       " 140: 'dogfighting',\n",
       " 141: 'operation',\n",
       " 142: 'gamble',\n",
       " 143: 'due',\n",
       " 144: 'federal',\n",
       " 145: 'court',\n",
       " 146: 'future',\n",
       " 147: 'remains',\n",
       " 148: 'uncertain',\n",
       " 149: 'parents',\n",
       " 150: 'beam',\n",
       " 151: 'pride',\n",
       " 152: 'cant',\n",
       " 153: 'stop',\n",
       " 154: 'smiling',\n",
       " 155: 'outpouring',\n",
       " 156: 'support',\n",
       " 157: 'mom',\n",
       " 158: 'happy',\n",
       " 159: 'didnt',\n",
       " 160: 'know',\n",
       " 161: 'doburn',\n",
       " 162: 'center',\n",
       " 163: 'offered',\n",
       " 164: 'provide',\n",
       " 165: 'treatment',\n",
       " 166: 'reconstructive',\n",
       " 167: 'surgeries',\n",
       " 168: 'dad',\n",
       " 169: 'anything',\n",
       " 170: 'youssif',\n",
       " 171: 'aid',\n",
       " 172: 'workers',\n",
       " 173: 'increased',\n",
       " 174: 'cost',\n",
       " 175: 'living',\n",
       " 176: 'drive',\n",
       " 177: 'women',\n",
       " 178: 'working',\n",
       " 179: 'raise',\n",
       " 180: 'awareness',\n",
       " 181: 'problem',\n",
       " 182: 'iraqs',\n",
       " 183: 'political',\n",
       " 184: 'leaders',\n",
       " 185: 'iraqi',\n",
       " 186: 'mothers',\n",
       " 187: 'tell',\n",
       " 188: 'turned',\n",
       " 189: 'help',\n",
       " 190: 'feed',\n",
       " 191: 'everything',\n",
       " 192: 'one',\n",
       " 193: 'woman',\n",
       " 194: 'tomas',\n",
       " 195: 'medina',\n",
       " 196: 'caracas',\n",
       " 197: 'fugitive',\n",
       " 198: 'drug',\n",
       " 199: 'trafficking',\n",
       " 200: 'indictment',\n",
       " 201: 'el',\n",
       " 202: 'negro',\n",
       " 203: 'acacio',\n",
       " 204: 'allegedly',\n",
       " 205: 'helped',\n",
       " 206: 'manage',\n",
       " 207: 'extensive',\n",
       " 208: 'cocaine',\n",
       " 209: 'network',\n",
       " 210: 'justice',\n",
       " 211: 'department',\n",
       " 212: 'indicted',\n",
       " 213: 'colombian',\n",
       " 214: 'military',\n",
       " 215: 'attack',\n",
       " 216: 'guerrilla',\n",
       " 217: 'encampment',\n",
       " 218: 'tony',\n",
       " 219: 'battle',\n",
       " 220: 'cancer',\n",
       " 221: 'win',\n",
       " 222: 'job',\n",
       " 223: 'secretary',\n",
       " 224: 'dream',\n",
       " 225: 'leaving',\n",
       " 226: 'september',\n",
       " 227: '14',\n",
       " 228: 'succeeded',\n",
       " 229: 'dana',\n",
       " 230: 'perino',\n",
       " 231: 'antitank',\n",
       " 232: 'front',\n",
       " 233: 'jersey',\n",
       " 234: 'device',\n",
       " 235: 'handed',\n",
       " 236: 'army',\n",
       " 237: 'ordnance',\n",
       " 238: 'disposal',\n",
       " 239: 'unit',\n",
       " 240: 'capable',\n",
       " 241: 'reloaded',\n",
       " 242: 'experts',\n",
       " 243: 'address',\n",
       " 244: 'veterans',\n",
       " 245: 'foreign',\n",
       " 246: 'wars',\n",
       " 247: 'withdrawing',\n",
       " 248: 'vietnam',\n",
       " 249: 'emboldened',\n",
       " 250: 'todays',\n",
       " 251: 'terrorists',\n",
       " 252: 'speech',\n",
       " 253: 'latest',\n",
       " 254: 'white',\n",
       " 255: 'house',\n",
       " 256: 'attempt',\n",
       " 257: 'try',\n",
       " 258: 'reframe',\n",
       " 259: 'debate',\n",
       " 260: 'iraq',\n",
       " 261: 'cars',\n",
       " 262: 'loaded',\n",
       " 263: 'gasoline',\n",
       " 264: 'nails',\n",
       " 265: 'abandoned',\n",
       " 266: '52',\n",
       " 267: 'people',\n",
       " 268: '7',\n",
       " 269: '2005',\n",
       " 270: 'bombs',\n",
       " 271: 'exploded',\n",
       " 272: 'bus',\n",
       " 273: 'trains',\n",
       " 274: 'british',\n",
       " 275: 'capital',\n",
       " 276: 'wracked',\n",
       " 277: 'ira',\n",
       " 278: 'years',\n",
       " 279: 'werder',\n",
       " 280: 'bremen',\n",
       " 281: 'record',\n",
       " 282: '107',\n",
       " 283: 'million',\n",
       " 284: 'carlos',\n",
       " 285: 'alberto',\n",
       " 286: 'brazilian',\n",
       " 287: 'midfielder',\n",
       " 288: 'champions',\n",
       " 289: 'league',\n",
       " 290: 'fc',\n",
       " 291: 'porto',\n",
       " 292: 'january',\n",
       " 293: 'loan',\n",
       " 294: 'fluminense',\n",
       " 295: 'saturday',\n",
       " 296: 'anesthetized',\n",
       " 297: 'last',\n",
       " 298: 'problems',\n",
       " 299: '2000',\n",
       " 300: 'customers',\n",
       " 301: 'electricity',\n",
       " 302: 'power',\n",
       " 303: 'company',\n",
       " 304: 'magnitude',\n",
       " 305: '42',\n",
       " 306: 'quake',\n",
       " 307: 'set',\n",
       " 308: 'alarms',\n",
       " 309: 'dispatcher',\n",
       " 310: 'fairly',\n",
       " 311: 'mild',\n",
       " 312: 'immediate',\n",
       " 313: 'reports',\n",
       " 314: 'injuries',\n",
       " 315: 'damage',\n",
       " 316: 'centered',\n",
       " 317: 'eastnortheast',\n",
       " 318: '36',\n",
       " 319: 'deep',\n",
       " 320: 'lady',\n",
       " 321: 'deeply',\n",
       " 322: 'saddened',\n",
       " 323: 'tragedy',\n",
       " 324: 'mine',\n",
       " 325: 'safety',\n",
       " 326: 'health',\n",
       " 327: 'administration',\n",
       " 328: 'weve',\n",
       " 329: 'run',\n",
       " 330: 'optionsthe',\n",
       " 331: 'six',\n",
       " 332: 'men',\n",
       " 333: 'trapped',\n",
       " 334: 'underground',\n",
       " 335: 'august',\n",
       " 336: '6',\n",
       " 337: 'seven',\n",
       " 338: 'bore',\n",
       " 339: 'holes',\n",
       " 340: 'drilled',\n",
       " 341: 'mountain',\n",
       " 342: 'signs',\n",
       " 343: 'life',\n",
       " 344: 'bomb',\n",
       " 345: 'victims',\n",
       " 346: 'waiting',\n",
       " 347: 'presidential',\n",
       " 348: 'visit',\n",
       " 349: 'blast',\n",
       " 350: 'went',\n",
       " 351: 'minutes',\n",
       " 352: 'presidents',\n",
       " 353: 'arrival',\n",
       " 354: 'algeria',\n",
       " 355: 'faces',\n",
       " 356: 'islamic',\n",
       " 357: 'insurgency',\n",
       " 358: 'al',\n",
       " 359: 'qaedaaffiliated',\n",
       " 360: 'claimed',\n",
       " 361: 'attacks',\n",
       " 362: 'agreed',\n",
       " 363: 'fiveyear',\n",
       " 364: 'los',\n",
       " 365: 'angeles',\n",
       " 366: 'galaxy',\n",
       " 367: 'took',\n",
       " 368: 'effect',\n",
       " 369: '1',\n",
       " 370: '2007',\n",
       " 371: 'former',\n",
       " 372: 'english',\n",
       " 373: 'captain',\n",
       " 374: 'meet',\n",
       " 375: 'unveil',\n",
       " 376: 'shirt',\n",
       " 377: 'number',\n",
       " 378: 'look',\n",
       " 379: 'footballer',\n",
       " 380: 'fashion',\n",
       " 381: 'icon',\n",
       " 382: 'global',\n",
       " 383: 'phenomenon',\n",
       " 384: 'colony',\n",
       " 385: 'millions',\n",
       " 386: 'scientists',\n",
       " 387: 'suspect',\n",
       " 388: 'virus',\n",
       " 389: 'may',\n",
       " 390: 'combine',\n",
       " 391: 'factors',\n",
       " 392: 'colonies',\n",
       " 393: 'cropped',\n",
       " 394: 'imported',\n",
       " 395: 'australia',\n",
       " 396: 'billion',\n",
       " 397: 'crops',\n",
       " 398: 'year',\n",
       " 399: 'dependent',\n",
       " 400: 'pollination',\n",
       " 401: 'savers',\n",
       " 402: 'leading',\n",
       " 403: 'uk',\n",
       " 404: 'mortgage',\n",
       " 405: 'lined',\n",
       " 406: 'accounts',\n",
       " 407: 'northern',\n",
       " 408: 'rock',\n",
       " 409: 'bailed',\n",
       " 410: 'england',\n",
       " 411: 'day',\n",
       " 412: 'earlier',\n",
       " 413: 'reassurances',\n",
       " 414: 'banks',\n",
       " 415: 'safe',\n",
       " 416: 'gone',\n",
       " 417: 'unheeded',\n",
       " 418: 'many'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_target_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
